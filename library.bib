@report{Meriem2022,
   abstract = {In teaching environments, student facial expressions are a clue to the traditional classroom teacher in gauging students' level of concentration in the course. With the rapid development of information technology, e-learning will take off because students can learn anytime, anywhere and anytime they feel comfortable. And this gives the possibility of self-learning. Analyzing student concentration can help improve the learning process. When the student is working alone on a computer in an e-learning environment, this task is particularly challenging to accomplish. Due to the distance between the teacher and the students, face-to-face communication is not possible in an e-learning environment. It is proposed in this article to use transfer learning and data augmentation techniques to determine the concentration level of learners from their facial expressions in real time. We found that expressed emotions correlate with students' concentration, and we designed three distinct levels of concentration (highly concentrated, nominally concentrated, and not at all concentrated).},
   author = {Bouhlal Meriem and Habib Benlahmar and Mohamed Amine Naji and Elfilali Sanaa and Kaiss Wijdane},
   issue = {1},
   journal = {IJACSA) International Journal of Advanced Computer Science and Applications},
   keywords = {Emotion recognition,data augmentation,level of concentration,transfer learning},
   title = {Determine the Level of Concentration of Students in Real Time from their Facial Expressions},
   volume = {13},
   url = {www.ijacsa.thesai.org},
   year = {2022},
}
@inproceedings{Sharma2019,
   abstract = {Analysis of student concentration can help to enhance the learning process. Emotions are directly related and directly reflect students’ concentration. This task is particularly difficult to implement in an e-learning environment, where the student stands alone in front of a computer. In this paper, a prototype system is proposed to figure out the concentration level in real-time from the expressed facial emotions during a lesson. An experiment was performed to evaluate the prototype system that was implemented using a client-side application that uses the C\# code available in Microsoft Azure Emotion API. We have found that the emotions expressed are correlated with the concentration of the students, and devised three distinct levels of concentration (high, medium, and low).},
   author = {Prabin Sharma and Meltem Esengönül and Salik Ram Khanal and Tulasi Tiwari Khanal and Vitor Filipe and Manuel J.C.S. Reis},
   doi = {10.1007/978-3-030-20954-4_40},
   isbn = {9783030209537},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Computer vision,Facial emotion recognition,Facial expression recognition,Student concentration},
   pages = {529-538},
   publisher = {Springer Verlag},
   title = {Student concentration evaluation index in an E-learning context using facial emotion analysis},
   volume = {993},
   year = {2019},
}
@inproceedings{Mayya2016,
   abstract = {Face depicts a wide range of information about identity, age, sex, race as well as emotional and mental state. Facial expressions play crucial role in social interactions and commonly used in the behavioral interpretation of emotions. Automatic facial expression recognition is one of the interesting and challenging problem in computer vision due to its potential applications such as Human Computer Interaction(HCI), behavioral science, video games etc. In this paper, a novel method for automatically recognizing facial expressions using Deep Convolutional Neural Network(DCNN) features is proposed. The proposed model focuses on recognizing the facial expressions of an individual from a single image. The feature extraction time is significantly reduced due to the usage of general purpose graphic processing unit (GPGPU). From an evaluation on two publicly available facial expression datasets, we have found that using DCNN features, we can achieve the state-of-the-art recognition rate.},
   author = {Veena Mayya and Radhika M. Pai and M. M. Manohara Pai},
   doi = {10.1016/j.procs.2016.07.233},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {CK+,Computer Vision,Confusion Matrix,Deep Convolutional Neural Network,Facial expression recognition,JAFFE,Machine Learning,Support vector machine},
   pages = {453-461},
   publisher = {Elsevier B.V.},
   title = {Automatic Facial Expression Recognition Using DCNN},
   volume = {93},
   year = {2016},
}
@article{Meriem2022,
   abstract = {In teaching environments, student facial expressions are a clue to the traditional classroom teacher in gauging students' level of concentration in the course. With the rapid development of information technology, e-learning will take off because students can learn anytime, anywhere and anytime they feel comfortable. And this gives the possibility of self-learning. Analyzing student concentration can help improve the learning process. When the student is working alone on a computer in an e-learning environment, this task is particularly challenging to accomplish. Due to the distance between the teacher and the students, face-to-face communication is not possible in an e-learning environment. It is proposed in this article to use transfer learning and data augmentation techniques to determine the concentration level of learners from their facial expressions in real time. We found that expressed emotions correlate with students' concentration, and we designed three distinct levels of concentration (highly concentrated, nominally concentrated, and not at all concentrated)},
   author = {Bouhlal Meriem and Habib Benlahmar and Mohamed Amine Naji and Elfilali Sanaa and Kaiss Wijdane},
   doi = {10.14569/IJACSA.2022.0130119},
   issn = {21565570},
   issue = {1},
   journal = {International Journal of Advanced Computer Science and Applications},
   keywords = {Data augmentation,Emotion recognition,Level of concentration,Transfer learning},
   pages = {159-166},
   publisher = {Science and Information Organization},
   title = {Determine the Level of Concentration of Students in Real Time from their Facial Expressions},
   volume = {13},
   year = {2022},
}
@inproceedings{Cha2015,
   author = {Seunghui Cha and Wookhyun Kim},
   doi = {10.14257/astl.2015.92.15},
   month = {4},
   pages = {72-76},
   publisher = {Science \& Engineering Research Support soCiety},
   title = {Analyze the learner's concentration using detection of facial feature points},
   year = {2015},
}
@article{Li2020,
   abstract = {To detect the students' concentration state in classroom, a DS (Dempster-Shafer theory)-based evaluation algorithm is proposed by measuring the students' Euler angles of their facial attitude. The detection of facial attitude angles can be implemented under the surveillance video with lower pixels. Therefore, compared with other methods for students' concentration evaluation, the proposed algorithm can be applied directly in most classrooms by the support of existing monitoring equipment. By using DS theory to fuse the concentration state of each student, the curve of students' overall concentration score changing with time can be obtained to describe the overall classroom concentration state. The design of the algorithm is proved to be feasible and effective under the dataset provided by computer front camera. The realization of the overall function effect of the algorithm is tested under the 35-person classroom video dataset. Compared with the average score from the questionnaire given by 20 reviewers, the accuracy of the proposed algorithm is about 85.3\%.},
   author = {Simin Li and Yaping Dai and Kaoru Hirota and Zhe Zuo},
   doi = {10.20965/JACIII.2020.P0891},
   issn = {18838014},
   issue = {7},
   journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
   keywords = {Classroom surveillance video,Concentration evaluation,Dempster-Shafer theory,Facial attitude recognition},
   month = {12},
   pages = {891-899},
   publisher = {Fuji Technology Press},
   title = {A students' concentration evaluation algorithm based on facial attitude recognition via classroom surveillance video},
   volume = {24},
   year = {2020},
}
@article{Hsu2012,
   abstract = {A growing number of educational studies apply sensors to improve student learning in real classroom settings. However, how can sensors be integrated into classrooms to help instructors find out students' reading concentration rates and thus better increase learning effectiveness? The aim of the current study was to develop a reading concentration monitoring system for use with e-books in an intelligent classroom and to help instructors find out the students' reading concentration rates. The proposed system uses three types of sensor technologies, namely a webcam, heartbeat sensor, and blood oxygen sensor to detect the learning behaviors of students by capturing various physiological signals. An artificial bee colony (ABC) optimization approach is applied to the data gathered from these sensors to help instructors understand their students' reading concentration rates in a classroom learning environment. The results show that the use of the ABC algorithm in the proposed system can effectively obtain near-optimal solutions.The system has a user-friendly graphical interface, making it easy for instructors to clearly understand the reading status of their students © 2012 by the authors; licensee MDPI, Basel, Switzerland.},
   author = {Chia Cheng Hsu and Hsin Chin Chen and Yen Ning Su and Kuo Kuang Huang and Yueh Min Huang},
   doi = {10.3390/s121014158},
   issn = {14248220},
   issue = {10},
   journal = {Sensors (Switzerland)},
   keywords = {Artificial bee colony algorithm;e-books,Intelligent classroom,Reading concentration,Sensor technology},
   month = {10},
   pages = {14158-14178},
   pmid = {23202042},
   title = {Developing a reading concentration monitoring system by applying an artificial bee colony algorithm to e-books in an intelligent classroom},
   volume = {12},
   year = {2012},
}
@article{Liu2013,
   abstract = {During the learning process, whether students remain attentive throughout instruction generally influences their learning efficacy. If teachers can instantly identify whether students are attentive they can be suitably reminded to remain focused, thereby improving their learning effects. Traditional teaching methods generally require that teachers observe students’ expressions to determine whether they are attentively learning. However, this method is often inaccurate and increases the burden on teachers. With the development of electroencephalography (EEG) detection tools, mobile brainwave sensors have become mature and affordable equipment. Therefore, in this study, whether students are attentive or inattentive during instruction is determined by observing their EEG signals. Because distinguishing between attentiveness and inattentiveness is challenging, two scenarios were developed for this study to measure the subjects’ EEG signals when attentive and inattentive. After collecting EEG data using mobile sensors, various common features were extracted from the raw data. A support vector machine (SVM) classifier was used to calculate and analyze these features to identify the combination of features that best indicates whether students are attentive. Based on the experiment results, the method proposed in this study provides a classification accuracy of up to 76.82\%. The study results can be used as a reference for learning system designs in the future.},
   author = {Ning Han Liu and Cheng Yu Chiang and Hsuan Chin Chu},
   doi = {10.3390/s130810273},
   issn = {14248220},
   issue = {8},
   journal = {Sensors (Switzerland)},
   keywords = {Attention status,Electroencephalogram,Electroencephalography (EEG) classification,Support vector machine},
   pages = {10273-10286},
   pmid = {23939584},
   publisher = {MDPI AG},
   title = {Recognizing the degree of human attention using EEG signals from mobile sensors},
   volume = {13},
   year = {2013},
}

@report{Whitehill2014,
   abstract = {Student engagement is a key concept in contemporary education, where it is valued as a goal in its own right. In this paper we explore approaches for automatic recognition of engagement from students' facial expressions. We studied whether human observers can reliably judge engagement from the face; analyzed the signals observers use to make these judgments; and automated the process using machine learning. We found that human observers reliably agree when discriminating low versus high degrees of engagement (Cohen's κ = 0.96). When fine discrimination is required (4 distinct levels) the reliability decreases, but is still quite high (κ = 0.56). Furthermore, we found that engagement labels of 10-second video clips can be reliably predicted from the average labels of their constituent frames (Pearson r = 0.85), suggesting that static expressions contain the bulk of the information used by observers. We used machine learning to develop automatic engagement detectors and found that for binary classification (e.g., high engagement versus low engagement), automated engagement detectors perform with comparable accuracy to humans. Finally, we show that both human and automatic engagement judgments correlate with task performance. In our experiment, student post-test performance was predicted with comparable accuracy from engagement labels (r = 0.47) as from pre-test scores (r = 0.44).},
   author = {Jacob Whitehill and Zewelanji Serpell and Yi-Ching Lin and Aysha Foster and Javier R Movellan},
   keywords = {Index Terms-Student engagement,engagement recognition,facial actions,facial expression recognition,intelligent tutoring systems !},
   title = {TRANSACTIONS ON AFFECTIVE COMPUTING The Faces of Engagement: Automatic Recognition of Student Engagement from Facial Expressions},
}
@article{Bosch2016,
title = "Detecting student emotions in computer-enabled classrooms",
abstract = "Affect detection is a key component of intelligent educational interfaces that can respond to the affective states of students. We use computer vision, learning analytics, and machine learning to detect students' affect in the real-world environment of a school computer lab that contained as many as thirty students at a time. Students moved around, gestured, and talked to each other, making the task quite difficult. Despite these challenges, we were moderately successful at detecting boredom, confusion, delight, frustration, and engaged concentration in a manner that generalized across students, time, and demographics. Our model was applicable 98% of the time despite operating on noisy realworld data.",
author = "Nigel Bosch and D'Mello, {Sidney K.} and Baker, {Ryan S.} and Jaclyn Ocumpaugh and Valerie Shute and Matthew Ventura and Lubin Wang and Weinan Zhao",
year = "2016",
language = "English (US)",
volume = "2016-January",
pages = "4125--4129",
journal = "IJCAI International Joint Conference on Artificial Intelligence",
issn = "1045-0823",
note = "25th International Joint Conference on Artificial Intelligence, IJCAI 2016 ; Conference date: 09-07-2016 Through 15-07-2016",
}

@misc{deepface,
   author = {Sefik Serengil},
   journal = {PyPi},
   month = {5},
   day = {10},
   url = {https://pypi.org/project/deepface/},
   title = {deepface 0.0.75},
   year = {2022},
}
@misc{OpenCv-PnP,
   journal = {OpenCV},
   url = {https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html},
   title = {Perspective-n-Point (PnP) pose computation},
}

@book{Lucey2010,
   abstract = {Title from HTML contents p. (IEEE Xplore, viewed Feb. 23, 2011).},
   author = {Patrick Lucey and Jeffrey F. Cohn and Takeo Kanade and Jason Saragih and Zara Ambadar},
   isbn = {9781424470303},
   publisher = {IEEE},
   title = {Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on : date, 13-18 June 2010.},
   year = {2010},
}

@report{Dalmaijer2014,
   abstract = {The PyGaze toolbox is an open-source software package for Python, a high-level programming language. It is designed for creating eye-tracking experiments in Python syntax with the least possible effort, and offers programming ease and script readability without constraining functionality and flexibility. PyGaze can be used for visual and auditory stimulus presentation, for response collection via keyboard, mouse, joystick, and other external hardware, and for online detection of eye movements based on a custom algorithm. A wide range of eye-trackers of different brands (Eyelink, SMI, and Tobii systems) are supported. The novelty of PyGaze lies in providing an easy-to-use layer on top of the many different software libraries that are required for implementing eye-tracking experiments. Essentially, PyGaze is a software-bridge for eye-tracking research.},
   author = {Edwin S Dalmaijer and Sebastiaan Mathôt and Stefan Van Der Stigchel},
   isbn = {1342801304222},
   keywords = {PsychoPy,Python,eye tracking,gaze contingency,open-source software},
   title = {PYGAZE: AN OPEN-SOURCE TOOLBOX FOR EYE-TRACKING PyGaze: an open-source, cross-platform toolbox for minimal-effort programming of eye-tracking experiments},
   url = {http://link.springer.com/article/10.3758\%2Fs13428-013-0422-2},
   year = {2014},
}
@article{Zhang_2016,
	doi = {10.1109/lsp.2016.2603342},
  
	url = {https://doi.org/10.1109\%2Flsp.2016.2603342},
  
	year = 2016,
	month = {oct},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {23},
  
	number = {10},
  
	pages = {1499--1503},
  
	author = {Kaipeng Zhang and Zhanpeng Zhang and Zhifeng Li and Yu Qiao},
  
	title = {Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},
  
	journal = {{IEEE} Signal Processing Letters}
}
@article{Anh2019,
   abstract = {Automated learning analytics is becoming an essential topic in the educational area, which needs effective systems to monitor the learning process and provides feedback to the teacher. Recent advances in visual sensors and computer vision methods enable automated monitoring of behavior and affective states of learners at different levels, from university to pre-school. The objective of this research was to build an automatic system that allowed the faculties to capture and make a summary of student behaviors in the classroom as a part of data acquisition for the decision making process. The system records the entire session and identifies when the students pay attention in the classroom, and then reports to the facilities. Our design and experiments show that our system is more flexible and more accurate than previously published work.},
   author = {Bui Ngoc Anh and Ngo Tung Son and Phan Truong Lam and Le Phuong Chi and Nguyen Huu Tuan and Nguyen Cong Dat and Nguyen Huu Trung and Muhammad Umar Aftab and Tran Van Dinh},
   doi = {10.3390/app9224729},
   issn = {20763417},
   issue = {22},
   journal = {Applied Sciences (Switzerland)},
   keywords = {Classification,Face detection,Facial recognition,Gaze estimation,Student's behavior,Visual attention},
   month = {11},
   publisher = {MDPI AG},
   title = {A Computer-vision based application for student behavior monitoring in classroom},
   volume = {9},
   year = {2019},
}
@article{Cheng2021,
   abstract = {Gaze estimation reveals where a person is looking. It is an important clue for understanding human intention. The recent development of deep learning has revolutionized many computer vision tasks, the appearance-based gaze estimation is no exception. However, it lacks a guideline for designing deep learning algorithms for gaze estimation tasks. In this paper, we present a comprehensive review of the appearance-based gaze estimation methods with deep learning. We summarize the processing pipeline and discuss these methods from four perspectives: deep feature extraction, deep neural network architecture design, personal calibration as well as device and platform. Since the data pre-processing and post-processing methods are crucial for gaze estimation, we also survey face/eye detection method, data rectification method, 2D/3D gaze conversion method, and gaze origin conversion method. To fairly compare the performance of various gaze estimation approaches, we characterize all the publicly available gaze estimation datasets and collect the code of typical gaze estimation algorithms. We implement these codes and set up a benchmark of converting the results of different methods into the same evaluation metrics. This paper not only serves as a reference to develop deep learning-based gaze estimation methods but also a guideline for future gaze estimation research. Implemented methods and data processing codes are available at http://phi-ai.org/GazeHub.},
   author = {Yihua Cheng and Haofei Wang and Yiwei Bao and Feng Lu},
   month = {4},
   title = {Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark},
   url = {http://arxiv.org/abs/2104.12668},
   year = {2021},
}
@article{Ucar2022,
   abstract = {With COVID-19, formal education was interrupted in all countries and the importance of distance learning has increased. It is possible to teach any lesson with various communication tools but it is difficult to know how far this lesson reaches to the students. In this study, it is aimed to monitor the students in a classroom or in front of the computer with a camera in real time, recognizing their faces, their head poses, and scoring their distraction to detect student engagement based on their head poses and Eye Aspect Ratios. Distraction was determined by associating the students’ attention with looking at the teacher or the camera in the right direction. The success of the face recognition and head pose estimation was tested by using the UPNA Head Pose Database and, as a result of the conducted tests, the most successful result in face recognition was obtained with the Local Binary Patterns method with a 98.95\% recognition rate. In the classification of student engagement as Engaged and Not Engaged, support vector machine gave results with 72.4\% accuracy. The developed system will be used to recognize and monitor students in the classroom or in front of the computer, and to determine the course flow autonomously.},
   author = {Mustafa Uğur Uçar and Ersin Özdemir},
   doi = {10.3390/electronics11091500},
   issn = {20799292},
   issue = {9},
   journal = {Electronics (Switzerland)},
   keywords = {computer vision,engagement detection,eye aspect ratio,head pose estimation,machine learning},
   month = {5},
   publisher = {MDPI},
   title = {Recognizing Students and Detecting Student Engagement with Real-Time Image Processing},
   volume = {11},
   year = {2022},
}
@report{Kazemi2014,
   abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
   author = {Vahid Kazemi and Josephine Sullivan Kth},
   title = {One Millisecond Face Alignment with an Ensemble of Regression Trees},
   year = {2014},
}
@article{Ablavatski2020,
   abstract = {We present a simple, real-time approach for pupil tracking from live video on mobile devices. Our method extends a state-of-the-art face mesh detector with two new components: a tiny neural network that predicts positions of the pupils in 2D, and a displacement-based estimation of the pupil blend shape coefficients. Our technique can be used to accurately control the pupil movements of a virtual puppet, and lends liveliness and energy to it. The proposed approach runs at over 50 FPS on modern phones, and enables its usage in any real-time puppeteering pipeline.},
   author = {Artsiom Ablavatski and Andrey Vakunov and Ivan Grishchenko and Karthik Raveendran and Matsvei Zhdanovich},
   month = {6},
   title = {Real-time Pupil Tracking from Monocular Video for Digital Puppetry},
   url = {http://arxiv.org/abs/2006.11341},
   year = {2020},
}
@report{Krafka2016,
   abstract = {From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCap-ture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolu-tional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.},
   author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},
   title = {Eye Tracking for Everyone},
   url = {http://gazecapture.csail.mit.edu.},
   year = {2016},
}
@misc{Martinez2022,
   author = {Juan Cruz Martinez},
   journal = {https://livecodestream.dev/post/detecting-face-features-with-python/},
   month = {6},
   title = {Detecting Face Features with Python},
   url = {https://livecodestream.dev/post/detecting-face-features-with-python/},
   year = {2022},
}
@misc{Agarwal2020,
   author = {Vardan Agarwal},
   journal = {https://livecodestream.dev/post/detecting-face-features-with-python/},
   month = {7},
   day = {25},
   title = {Real-Time Head Pose Estimation in Python},
   url = {https://towardsdatascience.com/real-time-head-pose-estimation-in-python-e52db1bc606a},
   year = {2020},
}
@misc{Vladimirov2020,
   author = {Lyudmil Vladimirov},
   journal = {https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_saved_model_tf1.html#sphx-glr-download-auto-examples-plot-object-detection-saved-model-tf1-py},
   title = {Object Detection From TF1 Saved Model},
   url = {https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_saved_model_tf1.html#sphx-glr-download-auto-examples-plot-object-detection-saved-model-tf1-py},
   year = {2020},
}
@misc{Yuan2020,
   author = {Ann Yuan and Andrey Vakunov},
   journal = {https://blog.tensorflow.org/2020/11/iris-landmark-tracking-in-browser-with-MediaPipe-and-TensorFlowJS.html},
   month = {11},
   title = {Iris landmark tracking in the browser with MediaPipe and TensorFlow.js},
   url = {https://blog.tensorflow.org/2020/11/iris-landmark-tracking-in-browser-with-MediaPipe-and-TensorFlowJS.html},
   year = {2020},
}
@misc{Mallick2019,
   author = {Satya Mallick},
   journal = {https://learnopencv.com/gaze-tracking/},
   month = {11},
   title = {Gaze Tracking},
   url = {https://learnopencv.com/gaze-tracking/},
   year = {2019},
}
@article{Startsev2019,
   abstract = {Deep learning approaches have achieved breakthrough performance in various domains. However, the segmentation of raw eye-movement data into discrete events is still done predominantly either by hand or by algorithms that use hand-picked parameters and thresholds. We propose and make publicly available a small 1D-CNN in conjunction with a bidirectional long short-term memory network that classifies gaze samples as fixations, saccades, smooth pursuit, or noise, simultaneously assigning labels in windows of up to 1 s. In addition to unprocessed gaze coordinates, our approach uses different combinations of the speed of gaze, its direction, and acceleration, all computed at different temporal scales, as input features. Its performance was evaluated on a large-scale hand-labeled ground truth data set (GazeCom) and against 12 reference algorithms. Furthermore, we introduced a novel pipeline and metric for event detection in eye-tracking recordings, which enforce stricter criteria on the algorithmically produced events in order to consider them as potentially correct detections. Results show that our deep approach outperforms all others, including the state-of-the-art multi-observer smooth pursuit detector. We additionally test our best model on an independent set of recordings, where our approach stays highly competitive compared to literature methods.},
   author = {Mikhail Startsev and Ioannis Agtzidis and Michael Dorr},
   doi = {10.3758/s13428-018-1144-2},
   issn = {15543528},
   issue = {2},
   journal = {Behavior Research Methods},
   keywords = {Deep learning,Eye-movement classification,Smooth pursuit},
   month = {4},
   pages = {556-572},
   pmid = {30411227},
   publisher = {Springer New York LLC},
   title = {1D CNN with BLSTM for automated classification of fixations, saccades, and smooth pursuits},
   volume = {51},
   year = {2019},
}
@article{Du2014,
   abstract = {Understanding the different categories of facial expressions of emotion regularly used by us is essential to gain insights into human cognition and affect as well as for the design of computational models and perceptual interfaces. Past research on facial expressions of emotion has focused on the study of six basic categories-happiness, surprise, anger, sadness, fear, and disgust. However, many more facial expressions of emotion exist and are used regularly by humans. This paper describes an important group of expressions, which we call compound emotion categories. Compound emotions are those that can be constructed by combining basic component categories to create new ones. For instance, happily surprised and angrily surprised are two distinct compound emotion categories. The present work defines 21 distinct emotion categories. Sample images of their facial expressions were collected from 230 human subjects. A Facial Action Coding System analysis shows the production of these 21 categories is different but consistent with the subordinate categories they represent (e.g., a happily surprised expression combines muscle movements observed in happiness and surprised). We show that these differences are sufficient to distinguish between the 21 defined categories. We then use a computational model of face perception to demonstrate that most of these categories are also visually discriminable from one another.},
   author = {Shichuan Du and Yong Tao and Aleix M. Martinez},
   doi = {10.1073/pnas.1322355111},
   issn = {10916490},
   issue = {15},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   keywords = {Action units,Categorization,Face recognition},
   month = {4},
   pmid = {24706770},
   publisher = {National Academy of Sciences},
   title = {Compound facial expressions of emotion},
   volume = {111},
   year = {2014},
}

@article{Meriem2022,
   abstract = {In teaching environments, student facial expressions are a clue to the traditional classroom teacher in gauging students' level of concentration in the course. With the rapid development of information technology, e-learning will take off because students can learn anytime, anywhere and anytime they feel comfortable. And this gives the possibility of self-learning. Analyzing student concentration can help improve the learning process. When the student is working alone on a computer in an e-learning environment, this task is particularly challenging to accomplish. Due to the distance between the teacher and the students, face-to-face communication is not possible in an e-learning environment. It is proposed in this article to use transfer learning and data augmentation techniques to determine the concentration level of learners from their facial expressions in real time. We found that expressed emotions correlate with students' concentration, and we designed three distinct levels of concentration (highly concentrated, nominally concentrated, and not at all concentrated)},
   author = {Bouhlal Meriem and Habib Benlahmar and Mohamed Amine Naji and Elfilali Sanaa and Kaiss Wijdane},
   doi = {10.14569/IJACSA.2022.0130119},
   issn = {21565570},
   issue = {1},
   journal = {International Journal of Advanced Computer Science and Applications},
   keywords = {Data augmentation,Emotion recognition,Level of concentration,Transfer learning},
   pages = {159-166},
   publisher = {Science and Information Organization},
   title = {Determine the Level of Concentration of Students in Real Time from their Facial Expressions},
   volume = {13},
   year = {2022},
}
@article{ekman1978facial,
  title={Facial action coding system},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Environmental Psychology \& Nonverbal Behavior},
  year={1978}
}
@misc{Tiwari2014,
   author = {Shantnu Tiwari},
   journal = {https://realpython.com/face-recognition-with-python/},
   month = {7},
   title = {Face Recognition with Python, in Under 25 Lines of Code},
   url = {https://realpython.com/face-recognition-with-python/},
   year = {2014},
}
@misc{Ivancic2019,
   author = {Kristijan Ivancic},
   journal = {https://realpython.com/traditional-face-detection-python/},
   month = {2},
   title = {Traditional Face Detection With Python},
   url = {https://realpython.com/traditional-face-detection-python/},
   year = {2019},
}
@misc{Rosebrock2017,
   author = {Adrian Rosebrock},
   journal = {https://pyimagesearch.com/2017/04/10/detect-eyes-nose-lips-jaw-dlib-opencv-python/},
   month = {4},
   title = {Detect eyes, nose, lips, and jaw with dlib, OpenCV, and Python},
   url = {https://pyimagesearch.com/2017/04/10/detect-eyes-nose-lips-jaw-dlib-opencv-python/},
   year = {2017},
}
@report{Benitez2016,
   abstract = {Research in face perception and emotion theory requires very large annotated databases of images of facial expressions of emotion. Annotations should include Action Units (AUs) and their intensities as well as emotion category. This goal cannot be readily achieved manually. Herein, we present a novel computer vision algorithm to annotate a large database of one million images of facial expressions of emotion in the wild (i.e., face images downloaded from the Internet). First, we show that this newly proposed algorithm can recognize AUs and their intensities reliably across databases. To our knowledge, this is the first published algorithm to achieve highly-accurate results in the recognition of AUs and their intensities across multiple databases. Our algorithm also runs in real-time (>30 images/second), allowing it to work with large numbers of images and video sequences. Second, we use WordNet to download 1,000,000 images of facial expressions with associated emotion keywords from the Internet. These images are then automatically annotated with AUs, AU intensities and emotion categories by our algorithm. The result is a highly useful database that can be readily queried using semantic descriptions for applications in computer vision, af-fective computing, social and cognitive psychology and neu-roscience; e.g., "show me all the images with happy faces" or "all images with AU 1 at intensity c."},
   author = {C Fabian Benitez-Quiroz and Ramprakash Srinivasan and Aleix M Martinez},
   city = {Las Vegas, NV},
   institution = {IEEEConference on Computer Vision and Pattern Recognition (CVPR),},
   pages = {5562-5570},
   title = {EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild},
   year = {2016},
}

@misc{Gradilla2020,
   author = {Rosa Gradilla},
   journal = {https://medium.com/@iselagradilla94/how-to-build-a-face-detection-application-using-pytorch-and-opencv-d46b0866e4d6},
   month = {7},
   title = {How to build a Face Detection application using PyTorch and OpenCV},
   url = {https://medium.com/@iselagradilla94/how-to-build-a-face-detection-application-using-pytorch-and-opencv-d46b0866e4d6},
   year = {2020},
}
@misc{Dulcic2019,
   author = {Luka Dulčić},
   journal = {Arsfutura},
   title = {Face Recognition with FaceNet and MTCNN},
   url = {https://arsfutura.com/magazine/face-recognition-with-facenet-and-mtcnn/},
   year = {2019},
}
@misc{Gradilla2020_1,
   author = {Rosa Gradilla},
   journal = {https://medium.com/@iselagradilla94/multi-task-cascaded-convolutional-networks-mtcnn-for-face-detection-and-facial-landmark-alignment-7c21e8007923},
   month = {7},
   title = {Multi-task Cascaded Convolutional Networks (MTCNN) for Face Detection and Facial Landmark Alignment},
   url = {https://medium.com/@iselagradilla94/multi-task-cascaded-convolutional-networks-mtcnn-for-face-detection-and-facial-landmark-alignment-7c21e8007923},
   year = {2020},
}
@misc{Khalid2021,
   author = {Irfan Alghani Khalid},
   journal = {https://towardsdatascience.com/face-landmark-detection-using-python-1964cb620837},
   month = {9},
   title = {Face Landmark Detection using Python},
   url = {https://towardsdatascience.com/face-landmark-detection-using-python-1964cb620837},
   year = {2021},
}
@misc{Allabadi2019,
   author = {B. Allabadi},
   journal = {https://www.kaggle.com/code/basharallabadi/yolov2-vs-faced-vs-blazeface-vs-mtcnn/notebook},
   title = {YOLOv2 vs Faced vs Blazeface vs MTCNN},
   url = {https://www.kaggle.com/code/basharallabadi/yolov2-vs-faced-vs-blazeface-vs-mtcnn/notebook},
   year = {2019},
}
@misc{Agarwal2020,
   author = {Vardan Agarwal},
   journal = {https://towardsdatascience.com/real-time-head-pose-estimation-in-python-e52db1bc606a},
   month = {7},
   title = {Real-Time Head Pose Estimation in Python},
   url = {https://towardsdatascience.com/real-time-head-pose-estimation-in-python-e52db1bc606a},
   year = {2020},
}
@misc{Rupesh2021,
   author = {Rupesh},
   journal = {https://rupeshthetech.medium.com/face-detection-models-and-their-performance-comparison-eb8da55f328c},
   month = {2},
   title = {Face Detection Models and their Performance Comparison},
   url = {https://rupeshthetech.medium.com/face-detection-models-and-their-performance-comparison-eb8da55f328c},
   year = {2021},
}
@article{Qi2021,
   abstract = {Tremendous progress has been made on face detection in recent years using convolutional neural networks. While many face detectors use designs designated for detecting faces, we treat face detection as a generic object detection task. We implement a face detector based on the YOLOv5 object detector and call it YOLO5Face. We make a few key modifications to the YOLOv5 and optimize it for face detection. These modifications include adding a five-point landmark regression head, using a stem block at the input of the backbone, using smaller-size kernels in the SPP, and adding a P6 output in the PAN block. We design detectors of different model sizes, from an extra-large model to achieve the best performance to a super small model for real-time detection on an embedded or mobile device. Experiment results on the WiderFace dataset show that on VGA images, our face detectors can achieve state-of-the-art performance in almost all the Easy, Medium, and Hard subsets, exceeding the more complex designated face detectors. The code is available at \url\{https://github.com/deepcam-cn/yolov5-face\}},
   author = {Delong Qi and Weijun Tan and Qi Yao and Jingfeng Liu},
   month = {5},
   title = {YOLO5Face: Why Reinventing a Face Detector},
   url = {http://arxiv.org/abs/2105.12931},
   year = {2021},
}
@misc{Tiwari2014,
   author = {Shantnu Tiwari},
   journal = {https://realpython.com/face-detection-in-python-using-a-webcam/},
   month = {7},
   title = {Face Detection in Python Using a Webcam},
   url = {https://realpython.com/face-detection-in-python-using-a-webcam/},
   year = {2014},
}
@misc{Durgan2021,
   author = {Shawn Durgan},
   journal = {https://morioh.com/p/b101507afdf5},
   title = {A Real-Time, High Accuracy Face Detection with Yolov5},
   url = {https://morioh.com/p/b101507afdf5},
   year = {2021},
}
@misc{GTL2022,
   author = {Great Learning Team},
   journal = {https://www.mygreatlearning.com/blog/face-recognition/},
   month = {10},
   title = {Face Recognition with Python and OpenCV},
   url = {https://www.mygreatlearning.com/blog/face-recognition/},
   year = {2022},
}
@misc{GTL2022_1,
   author = {Great Learning Team},
   journal = {https://www.mygreatlearning.com/blog/yolo-object-detection-using-opencv/?highlight=object},
   month = {3},
   title = {YOLO object detection using OpenCV},
   url = {https://www.mygreatlearning.com/blog/yolo-object-detection-using-opencv/?highlight=object},
   year = {2022},
}
@misc{Gulati2022,
   author = {Aman Preet Gulati},
   journal = {https://www.analyticsvidhya.com/blog/2022/03/facial-landmarks-detection-using-mediapipe-library/},
   month = {3},
   title = {Facial Landmarks Detection Using Mediapipe Library},
   url = {https://www.analyticsvidhya.com/blog/2022/03/facial-landmarks-detection-using-mediapipe-library/},
   year = {2022},
}
@misc{GoogleFace,
   author = {GOOGLE},
   journal = {GOOGLE LLC},
   title = {MediaPipe Face Detection},
   url = {https://google.github.io/mediapipe/solutions/face_detection.html},
}
@article{Lugaresi2019,
   abstract = {Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.},
   author = {Camillo Lugaresi and Jiuqiang Tang and Hadon Nash and Chris McClanahan and Esha Uboweja and Michael Hays and Fan Zhang and Chuo-Ling Chang and Ming Guang Yong and Juhyun Lee and Wan-Teh Chang and Wei Hua and Manfred Georg and Matthias Grundmann},
   month = {6},
   title = {MediaPipe: A Framework for Building Perception Pipelines},
   url = {http://arxiv.org/abs/1906.08172},
   year = {2019},
}
@misc{Maindola2021,
   author = {Gaurav Maindola},
   journal = {https://machinelearningknowledge.ai/face-recognition-and-face-landmark-detection-in-tensorflow-js/},
   month = {5},
   title = {Face Recognition and Face Landmark Detection with Tensorflow.js BlazeFace},
   url = {https://machinelearningknowledge.ai/face-recognition-and-face-landmark-detection-in-tensorflow-js/},
   year = {2021},
}
@misc{Guse2020,
   author = {Justin Güse},
   journal = { Towards Data Science},
   month = {4},
   title = {Face Detection using MTCNN — a guide for face extraction with a focus on speed},
   url = {https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49},
   year = {2020},
}
@misc{GoogleIris,
   author = {GOOGLE},
   journal = {GOOGLE LLC},
   title = {MediaPipe Iris},
   url = {https://google.github.io/mediapipe/solutions/iris.html},
}
@misc{Yuan2020,
   author = {Ann Yuan and Andrey Vakunov},
   journal = {TensorFlow Blog},
   month = {3},
   title = {Face and hand tracking in the browser with MediaPipe and TensorFlow.js},
   url = {https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html},
   year = {2020},
}
@misc{Breuss2021,
   author = {Martin Breuss},
   journal = {Real Python},
   month = {2},
   title = {Python Web Applications: Deploy Your Script as a Flask App},
   url = {https://realpython.com/python-web-applications/},
   year = {2021},
}
@misc{DTGLAND,
   author = {Datagen},
   journal = {Datagen},
   title = {Facial Landmarks: Use Cases, Datasets, and a Quick Tutorial},
   url = {https://datagen.tech/guides/face-recognition/facial-landmarks/},
}
@article{Siam2022,
   abstract = {Emotion recognition is one of the trending research fields. It is involved in several applications. Its most interesting applications include robotic vision and interactive robotic communication. Human emotions can be detected using both speech and visual modalities. Facial expressions can be considered as ideal means for detecting the persons' emotions. This paper presents a real-time approach for implementing emotion detection and deploying it in the robotic vision applications. The proposed approach consists of four phases: preprocessing, key point generation, key point selection and angular encoding, and classification. The main idea is to generate key points using MediaPipe face mesh algorithm, which is based on real-time deep learning. In addition, the generated key points are encoded using a sequence of carefully designed mesh generator and angular encoding modules. Furthermore, feature decomposition is performed using Principal Component Analysis (PCA). This phase is deployed to enhance the accuracy of emotion detection. Finally, the decomposed features are enrolled into a Machine Learning (ML) technique that depends on a Support Vector Machine (SVM), k-Nearest Neighbor (KNN), Naïve Bayes (NB), Logistic Regression (LR), or Random Forest (RF) classifier. Moreover, we deploy a Multilayer Perceptron (MLP) as an efficient deep neural network technique. The presented techniques are evaluated on different datasets with different evaluation metrics. The simulation results reveal that they achieve a superior performance with a human emotion detection accuracy of 97%, which ensures superiority among the efforts in this field.},
   author = {Ali I. Siam and Naglaa F. Soliman and Abeer D. Algarni and Fathi E. Abd El-Samie and Ahmed Sedik},
   doi = {10.1155/2022/8032673},
   issn = {16875273},
   journal = {Computational Intelligence and Neuroscience},
   pmid = {35154306},
   publisher = {Hindawi Limited},
   title = {Deploying Machine Learning Techniques for Human Emotion Detection},
   volume = {2022},
   year = {2022},
}
@misc{Singh2021,
   author = {Rahulraj Singh},
   journal = {Towards Data Science},
   month = {7},
   title = {The Ultimate Guide to Emotion Recognition from Facial Expressions using Python},
   url = {https://towardsdatascience.com/the-ultimate-guide-to-emotion-recognition-from-facial-expressions-using-python-64e58d4324ff},
   year = {2021},
}
@misc{Choudhury2020,
   author = {Ambika Choudhury},
   journal = {DEVELOPERS CORNER},
   month = {9},
   title = {Top 8 Datasets Available For Emotion Detection},
   url = {https://analyticsindiamag.com/top-8-datasets-available-for-emotion-detection/},
   year = {2020},
}
@misc{Brownlee2022,
   author = {Jason Brownlee},
   journal = {Machine Learning Mastery},
   month = {6},
   title = {Your First Deep Learning Project in Python with Keras Step-by-Step},
   url = {https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/},
   year = {2022},
}
@misc{Birost2022,
   author = {Birost},
   journal = {Birost},
   title = {Face pose estimation (calculate Euler angle)},
   url = {https://blog.birost.com/a?ID=01450-d1380ccc-2edd-406d-b491-afd6782e9b41},
   year = {2022},
}
@misc{Mallick2016,
   author = {Satya Mallick},
   journal = {LearnOpenCV},
   month = {9},
   title = {Head Pose Estimation using OpenCV and Dlib},
   url = {https://learnopencv.com/head-pose-estimation-using-opencv-and-dlib/},
   year = {2016},
}
@misc{Khalid2021,
   author = {Irfan Alghani Khalid},
   journal = {Towards Data Science},
   month = {9},
   title = {Head Pose Estimation using Python},
   url = {https://towardsdatascience.com/head-pose-estimation-using-python-d165d3541600},
   year = {2021},
}
@misc{Srivastava2021,
   author = {Vyom Srivastava},
   journal = {WordPress},
   month = {12},
   title = {Emotion Detection using Python},
   url = {https://geekyhumans.com/emotion-detection-using-python-and-deepface/},
   year = {2021},
}
@misc{Bimbramayank2020,
   author = {Bimbramayank},
   month = {7},
   title = {Real Time Facial Expressions/Emotions Recognition on a Web Interface using Python},
   url = {https://mayankbimbra.medium.com/real-time-facial-expressions-emotions-recognition-on-a-web-interface-using-python-b42f58a25780},
   year = {2020},
}
@misc{Serengil2022,
   author = {Sefik Ilkin Serengil},
   month = {5},
   title = {Normalization for Facial Recognition with MediaPipe},
   url = {https://sefiks.com/2022/05/29/normalization-for-facial-recognition-with-mediapipe/},
   year = {2022},
}
@misc{Rosebrock2018,
   author = {Adrian Rosebrock},
   journal = {PyImageSearch},
   month = {6},
   title = {Face recognition with OpenCV, Python, and deep learning},
   url = {https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/},
   year = {2018},
}
@misc{Rosebrock2017,
   author = {Adrian Rosebrock},
   journal = {PyImageSearch},
   month = {4},
   title = {Facial landmarks with dlib, OpenCV, and Python},
   url = {https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/},
   year = {2017},
}
@misc{Agarwal2020,
   author = {Vardan Agarwal},
   journal = {Towards Data Science},
   month = {7},
   title = {Face Detection Models: Which to Use and Why?},
   url = {https://towardsdatascience.com/face-detection-models-which-to-use-and-why-d263e82c302c},
   year = {2020},
}
@misc{Zubair2021,
   author = {Md. Zubair},
   journal = {Towards Data Science},
   month = {7},
   title = {Write a Few Lines of Code and Detect Faces, Draw Landmarks from Complex Images ~MediaPipe},
   url = {https://towardsdatascience.com/write-a-few-lines-of-code-and-detect-faces-draw-landmarks-from-complex-images-mediapipe-932f07566d11},
   year = {2021},
}
@misc{GoogleHands,
   author = {GOOGLE},
   journal = {GOOGLE LLC},
   title = {MediaPipe Hands},
   url = {https://google.github.io/mediapipe/solutions/hands.html},
}
@misc{Mudgal2021,
   author = {Vaibhav Mudgal},
   journal = {Medium},
   month = {6},
   title = {Real-Time Gesture Recognition Using GOOGLE’S MediaPipe Hands — Add Your Own Gestures [Tutorial #1]},
   url = {https://mudgalvaibhav.medium.com/real-time-gesture-recognition-using-googles-mediapipe-hands-add-your-own-gestures-tutorial-1-dd7f14169c19},
   year = {2021},
}
